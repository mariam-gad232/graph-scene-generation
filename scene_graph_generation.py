# -*- coding: utf-8 -*-
"""scene graph generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hfZG0ondpS59nRrI77TMNguohk8Twyl
"""

# Cell 1: Install dependencies
!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
!pip install pyyaml==6.0  # Updated version that works with Python 3.10+
!pip install 'git+https://github.com/facebookresearch/detectron2.git'
!pip install datasets matplotlib

import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

from datasets import load_dataset
dataset = load_dataset("visual_genome", "relationships_v1.2.0", split="train[:100]")

import torch
import numpy as np
from datasets import load_dataset
import matplotlib.pyplot as plt
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.modeling import build_model
from detectron2.checkpoint import DetectionCheckpointer

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg.MODEL.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

model = build_model(cfg)
DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)
model.eval()

from detectron2.data import MetadataCatalog, DatasetMapper
cfg.DATASETS.TRAIN = ("visual_genome",)
MetadataCatalog.get("visual_genome").set(thing_classes=[""])  # Placeholder
model.meta = MetadataCatalog.get("visual_genome")

VG_CLASSES = [
    'person', 'clothing', 'dog', 'cat', 'tree', 'car', 'building', 'book',
    'chair', 'table', 'computer', 'cup', 'food', 'plant', 'vehicle', 'sign',
    'door', 'window', 'shoe', 'grass', 'ground', 'road', 'water', 'flower',
    'fruit', 'animal', 'boat', 'plane', 'train', 'bicycle', 'bag', 'phone',
    'clock', 'light', 'bed', 'desk', 'shelf', 'cabinet', 'mirror', 'towel',
    'box', 'paper', 'dish', 'utensil', 'bottle', 'glass', 'vegetable',
    'electronic', 'instrument', 'tool'
]  # Top 50 classes for demo

VG_RELATIONS = [
    'holding', 'wearing', 'sitting on', 'standing on', 'next to',
    'looking at', 'under', 'attached to', 'riding', 'eating', 'drinking',
    'flying', 'playing', 'covering', 'carrying', 'with', 'on back of',
    'walking on', 'lying on', 'in front of', 'behind', 'parked on',
    'growing on', 'hanging from', 'over', 'brushing', 'watching',
    'using', 'touching', 'feeding', 'reading', 'cooking', 'holding hand of'
]  # Top 33 relations

import cv2
import matplotlib.pyplot as plt
def draw_relationships(image, outputs, threshold=0.7):
    fig = plt.figure(figsize=(15, 10))
    frame = image.copy()

    # Convert to RGB if needed
    if len(frame.shape) == 2:
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
    elif frame.shape[2] == 4:
        frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)

    # Extract instances
    instances = outputs["instances"]
    boxes = instances.pred_boxes.tensor.cpu().numpy()
    scores = instances.scores.cpu().numpy()
    labels = instances.pred_classes.cpu().numpy()

    # Draw objects
    for box, score, label in zip(boxes, scores, labels):
        if score < threshold:
            continue
        x1, y1, x2, y2 = map(int, box)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
        class_name = VG_CLASSES[label] if label < len(VG_CLASSES) else f"Obj {label}"
        cv2.putText(frame, f"{class_name} {score:.2f}", (x1, y1-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1)

    # Simple relationship detection (nearest neighbor demo)
    valid_boxes = [box for box, score in zip(boxes, scores) if score >= threshold]
    for i in range(len(valid_boxes)):
        for j in range(i+1, len(valid_boxes)):
            box1 = valid_boxes[i]
            box2 = valid_boxes[j]

            # Calculate centers and convert to integers
            center1 = (int((box1[0]+box1[2])//2), int((box1[1]+box1[3])//2))
            center2 = (int((box2[0]+box2[2])//2), int((box2[1]+box2[3])//2))

            # Draw relationship line
            cv2.line(frame, center1, center2, (0,0,255), 1)

            # Draw relationship text
            mid_point = ((center1[0] + center2[0])//2, (center1[1] + center2[1])//2)
            cv2.putText(frame, "related", mid_point,
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)

    plt.imshow(frame)
    plt.axis('off')
    plt.show()

def process_image(image):
    # Convert to numpy array
    image = np.array(image.convert("RGB"))
    height, width = image.shape[:2]

    # Prepare inputs
    inputs = {
        "image": torch.as_tensor(image.astype("float32").transpose(2, 0, 1)),
        "height": height,
        "width": width
    }

    # Run model
    with torch.no_grad():
        outputs = model([inputs])[0]

    return outputs

sample_idx = 0  # Try different indices (0-99)
sample = dataset[sample_idx]

# Original image
plt.figure(figsize=(10, 8))
plt.imshow(sample['image'])
plt.title("Original Image")
plt.axis('off')
plt.show()

# Process and visualize
outputs = process_image(sample['image'])
draw_relationships(np.array(sample['image']), outputs)

torch.save(model.state_dict(), "scene_graph_model.pth")
print("Model saved!")